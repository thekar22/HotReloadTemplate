{"version":3,"sources":["../import/scraper.js"],"names":["template","title","description","urls","scrapeURL","url","then","objectFromScraping","e","console","log","requestURL","error","response","body","statusCode","onAllURLsFetched","data","dataPromises","map","Promise","all"],"mappings":";;AAAA;;;;AACA;;;;;;AAAiC;;AAEjC;AACA,IAAMA,WAAW;AAChBC,QAAO,OADS;AAEhBC,cAAa;AAFG,CAAjB,C,CAJ8B;;;AAY9B;AACA,IAAIC,OAAO,CACV,wBADU,EAEV,0BAFU,EAGV,wBAHU,CAAX;;AASA;AACA,SAASC,SAAT,CAAmBC,GAAnB,EAAwBL,QAAxB,EAAkC;AACjC,KAAI;AACH,SAAO,wBAASK,GAAT,EAAcL,QAAd,EAAwBM,IAAxB,CAA6B,8BAAsB;AACzD;;;AAGA,UAAOC,kBAAP;AACA,GALM,CAAP;AAMA,EAPD,CAOE,OAAMC,CAAN,EAAS;AACVC,UAAQC,GAAR,CAAY,kBAAZ,EAAgCF,CAAhC;AACA;AACD;;AAED;AACA,SAASG,UAAT,CAAoBN,GAApB,EAAyB;AACxB,KAAI;AACH,yBAAQA,GAAR,EAAa,UAACO,KAAD,EAAQC,QAAR,EAAkBC,IAAlB,EAA2B;AACvC,OAAIF,SAASC,SAASE,UAAT,KAAwB,GAArC,EAA0C;AACzC;AACA;;AAED,OAAI,CAEH,CAFD,CAEE,OAAOP,CAAP,EAAU,CAEX;AACD,GAVD;AAWA,EAZD,CAYE,OAAMA,CAAN,EAAS;AACVC,UAAQC,GAAR,CAAY,oBAAZ;AACA;AAED;;AAED,SAASM,gBAAT,CAA0BC,IAA1B,EAAgC;AAC/B;AACAR,SAAQC,GAAR,CAAYO,IAAZ;;AAEA;;;AAIA;;AAED;AACA,IAAI;AACH,KAAMC,eAAef,KAAKgB,GAAL,CAAS;AAAA,SAAOf,UAAUC,GAAV,EAAeL,QAAf,CAAP;AAAA,EAAT,CAArB;AACAoB,SAAQC,GAAR,CAAYH,YAAZ,EAA0BZ,IAA1B,CAA+B,gBAAQ;AACtCU,mBAAiBC,IAAjB;AACA,EAFD;AAGA,CALD,CAKE,OAAMT,CAAN,EAAS;AACVC,SAAQC,GAAR,CAAY,eAAZ,EAA6BF,CAA7B;AACA","file":"scraper.js","sourcesContent":["import request from 'request' // to make http requests\nimport scrapeIt from 'scrape-it' // to scrape url\n\n// specify here the object-dom transformation you wish to scrape \nconst template = {\n\ttitle: 'title',\n\tdescription: 'div#someId',\n\t/*\n\t\tPopulate this object with whatever fields you want to pull\n\t*/\n}\n\n// list of predictable urls to scrape (perhaps, urls will comprise of same url with differing pagination values in the query string)\nvar urls = [\n\t\"https://www.google.com\",\n\t\"https://www.facebook.com\",\n\t\"https://www.github.com\",\n\t/*\n\t\tPopulate this list with your own urls you need to scrape\n\t*/\n]\n\n// scrape url with the given object template\nfunction scrapeURL(url, template) {\n\ttry {\n\t\treturn scrapeIt(url, template).then(objectFromScraping => {\n\t\t\t/*\n\t\t\t\tany post-processing for each url scrape can go here\n\t\t\t*/\n\t\t\treturn objectFromScraping;\n\t\t})\n\t} catch(e) {\n\t\tconsole.log('failure scraping', e)\n\t}\n}\n\n// get response of URL\nfunction requestURL(url) {\n\ttry {\n\t\trequest(url, (error, response, body) => {\n\t\t\tif (error || response.statusCode !== 200) {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\ttry {\n\n\t\t\t} catch (e) {\n\n\t\t\t}\n\t\t})\t\n\t} catch(e) {\n\t\tconsole.log('failure requesting')\n\t}\n\t\n}\n\nfunction onAllURLsFetched(data) {\n\t// data contains all object results from all scraped urls \n\tconsole.log(data);\n\n\t/*\n\t\tDo something with your scraped data here\n\t*/\n\n}\n\n// scrape all URLs in urls and \ntry {\n\tconst dataPromises = urls.map(url => scrapeURL(url, template))\n\tPromise.all(dataPromises).then(data => {\n\t\tonAllURLsFetched(data)\n\t})\n} catch(e) {\n\tconsole.log(\"scrape failed\", e)\n}\n"]}